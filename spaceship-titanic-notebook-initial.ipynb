{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-06-30T19:06:05.090114Z","iopub.execute_input":"2022-06-30T19:06:05.090651Z","iopub.status.idle":"2022-06-30T19:06:05.118884Z","shell.execute_reply.started":"2022-06-30T19:06:05.090541Z","shell.execute_reply":"2022-06-30T19:06:05.117962Z"},"editable":false,"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# Load the data\nFirst things first I got to load the data.","metadata":{"editable":false}},{"cell_type":"code","source":"#loading data\ndf_train = pd.read_csv('/kaggle/input/spaceship-titanic/train.csv')\ndf_test = pd.read_csv('/kaggle/input/spaceship-titanic/test.csv')\ndf_sub = pd.read_csv('/kaggle/input/spaceship-titanic/sample_submission.csv')","metadata":{"execution":{"iopub.status.busy":"2022-06-30T19:06:09.681438Z","iopub.execute_input":"2022-06-30T19:06:09.681970Z","iopub.status.idle":"2022-06-30T19:06:09.762246Z","shell.execute_reply.started":"2022-06-30T19:06:09.681922Z","shell.execute_reply":"2022-06-30T19:06:09.760645Z"},"editable":false,"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"df_test.info()","metadata":{"execution":{"iopub.status.busy":"2022-06-30T19:06:12.146545Z","iopub.execute_input":"2022-06-30T19:06:12.146904Z","iopub.status.idle":"2022-06-30T19:06:12.183827Z","shell.execute_reply.started":"2022-06-30T19:06:12.146875Z","shell.execute_reply":"2022-06-30T19:06:12.182543Z"},"editable":false,"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# Data Exploration\nNow we need to explore the data. Check for missing values, view descriptive statistics, check correlations and skew. ","metadata":{"editable":false}},{"cell_type":"code","source":"#check dataframe\ndf_train.head(5)","metadata":{"execution":{"iopub.status.busy":"2022-06-30T19:06:14.823971Z","iopub.execute_input":"2022-06-30T19:06:14.824678Z","iopub.status.idle":"2022-06-30T19:06:14.866892Z","shell.execute_reply.started":"2022-06-30T19:06:14.824620Z","shell.execute_reply":"2022-06-30T19:06:14.865544Z"},"editable":false,"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"#get shape (rows/columns)\ndf_train.shape","metadata":{"execution":{"iopub.status.busy":"2022-06-30T19:06:17.220135Z","iopub.execute_input":"2022-06-30T19:06:17.220523Z","iopub.status.idle":"2022-06-30T19:06:17.227525Z","shell.execute_reply.started":"2022-06-30T19:06:17.220484Z","shell.execute_reply":"2022-06-30T19:06:17.226488Z"},"editable":false,"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"#check for empty values and datatypes\ndf_train.info()","metadata":{"execution":{"iopub.status.busy":"2022-06-30T11:05:58.159767Z","iopub.execute_input":"2022-06-30T11:05:58.160426Z","iopub.status.idle":"2022-06-30T11:05:58.175429Z","shell.execute_reply.started":"2022-06-30T11:05:58.160392Z","shell.execute_reply":"2022-06-30T11:05:58.174279Z"},"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#display descriptive statistics\ndf_train.describe()\n#data is very skewed!","metadata":{"execution":{"iopub.status.busy":"2022-06-30T19:06:21.189283Z","iopub.execute_input":"2022-06-30T19:06:21.189800Z","iopub.status.idle":"2022-06-30T19:06:21.234521Z","shell.execute_reply.started":"2022-06-30T19:06:21.189754Z","shell.execute_reply":"2022-06-30T19:06:21.233553Z"},"editable":false,"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"#display correlations\ndf_train.corr(method='pearson')","metadata":{"execution":{"iopub.status.busy":"2022-06-30T11:06:06.00868Z","iopub.execute_input":"2022-06-30T11:06:06.009665Z","iopub.status.idle":"2022-06-30T11:06:06.03467Z","shell.execute_reply.started":"2022-06-30T11:06:06.009609Z","shell.execute_reply":"2022-06-30T11:06:06.033669Z"},"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.skew()","metadata":{"execution":{"iopub.status.busy":"2022-06-30T01:47:48.080481Z","iopub.execute_input":"2022-06-30T01:47:48.081951Z","iopub.status.idle":"2022-06-30T01:47:48.109029Z","shell.execute_reply.started":"2022-06-30T01:47:48.081897Z","shell.execute_reply":"2022-06-30T01:47:48.107224Z"},"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Visualization\nLets create some basic graphs to visualize our data","metadata":{"editable":false}},{"cell_type":"code","source":"#histogram overview\ndf_train.hist(figsize = (20, 10)) ","metadata":{"execution":{"iopub.status.busy":"2022-06-30T19:06:27.493121Z","iopub.execute_input":"2022-06-30T19:06:27.493525Z","iopub.status.idle":"2022-06-30T19:06:28.323501Z","shell.execute_reply.started":"2022-06-30T19:06:27.493492Z","shell.execute_reply":"2022-06-30T19:06:28.322140Z"},"editable":false,"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"df_train['RoomService'].hist(bins=[0, 20, 40])","metadata":{"execution":{"iopub.status.busy":"2022-06-30T19:06:30.925422Z","iopub.execute_input":"2022-06-30T19:06:30.925791Z","iopub.status.idle":"2022-06-30T19:06:31.218897Z","shell.execute_reply.started":"2022-06-30T19:06:30.925762Z","shell.execute_reply":"2022-06-30T19:06:31.217804Z"},"editable":false,"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"#density plot\ndf_train.plot(kind='density', figsize = (20, 10), subplots=True, layout=(3,3), sharex=False)","metadata":{"execution":{"iopub.status.busy":"2022-06-30T01:47:53.748263Z","iopub.execute_input":"2022-06-30T01:47:53.748664Z","iopub.status.idle":"2022-06-30T01:47:56.278163Z","shell.execute_reply.started":"2022-06-30T01:47:53.748633Z","shell.execute_reply":"2022-06-30T01:47:56.275888Z"},"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#box and whisker plot\ndf_train.plot(kind='box', subplots=True, figsize = (20, 10), layout=(3,3), sharex=False, sharey=False) ","metadata":{"execution":{"iopub.status.busy":"2022-06-29T10:37:42.439703Z","iopub.execute_input":"2022-06-29T10:37:42.440264Z","iopub.status.idle":"2022-06-29T10:37:43.288059Z","shell.execute_reply.started":"2022-06-29T10:37:42.440228Z","shell.execute_reply":"2022-06-29T10:37:43.286755Z"},"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns","metadata":{"execution":{"iopub.status.busy":"2022-06-30T11:37:03.003754Z","iopub.execute_input":"2022-06-30T11:37:03.004166Z","iopub.status.idle":"2022-06-30T11:37:03.48436Z","shell.execute_reply.started":"2022-06-30T11:37:03.004134Z","shell.execute_reply":"2022-06-30T11:37:03.483232Z"},"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(x='HomePlanet', hue= 'Transported', data=df_train) ","metadata":{"execution":{"iopub.status.busy":"2022-06-30T01:47:58.902764Z","iopub.execute_input":"2022-06-30T01:47:58.903632Z","iopub.status.idle":"2022-06-30T01:47:59.15284Z","shell.execute_reply.started":"2022-06-30T01:47:58.903583Z","shell.execute_reply":"2022-06-30T01:47:59.151157Z"},"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(x='CryoSleep', hue= 'Transported', data=df_train) ","metadata":{"execution":{"iopub.status.busy":"2022-06-29T15:25:23.775748Z","iopub.execute_input":"2022-06-29T15:25:23.77616Z","iopub.status.idle":"2022-06-29T15:25:23.959604Z","shell.execute_reply.started":"2022-06-29T15:25:23.77611Z","shell.execute_reply":"2022-06-29T15:25:23.958216Z"},"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(x='Destination', hue= 'Transported', data=df_train) ","metadata":{"execution":{"iopub.status.busy":"2022-06-29T10:37:53.40681Z","iopub.execute_input":"2022-06-29T10:37:53.407872Z","iopub.status.idle":"2022-06-29T10:37:53.616763Z","shell.execute_reply.started":"2022-06-29T10:37:53.4078Z","shell.execute_reply":"2022-06-29T10:37:53.615704Z"},"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(x='VIP', hue= 'Transported', data=df_train) ","metadata":{"execution":{"iopub.status.busy":"2022-06-29T10:37:55.894532Z","iopub.execute_input":"2022-06-29T10:37:55.894895Z","iopub.status.idle":"2022-06-29T10:37:56.116917Z","shell.execute_reply.started":"2022-06-29T10:37:55.894868Z","shell.execute_reply":"2022-06-29T10:37:56.116075Z"},"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Munging\nSo I can go about this a bunch of different ways. The fastest way is to drop everything that has an NA value and then run ML algorithms. Doing this takes you from 8700 to 6600 entries, 20-25% of data lost. Yikes that's a lot!So lets go through the columns on by one and try filling in missing values. There's a lot of \"NaN\" values in both object and float datatype columns. There's also a lot of \"0\" values in spending for various services, I'll have to decide if I treat that as a NA value or if 0 is the correct value.\n\nThe easiest way to go about filling NA values is to just use the mode for categoricals and median for numeric values. ","metadata":{"editable":false}},{"cell_type":"code","source":"df_train.info()","metadata":{"execution":{"iopub.status.busy":"2022-06-30T19:06:36.236154Z","iopub.execute_input":"2022-06-30T19:06:36.236976Z","iopub.status.idle":"2022-06-30T19:06:36.258103Z","shell.execute_reply.started":"2022-06-30T19:06:36.236933Z","shell.execute_reply":"2022-06-30T19:06:36.256891Z"},"editable":false,"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"#split cabin into 3 columns, there are 3 types of data contained in this\n\ndf_train[['Cabin1', 'Cabin2', 'Cabin3']] = df_train['Cabin'].str.split('/', expand = True)\n\n\n#create lists of categoricals/numerics/identifiers\ncategoricals = ['HomePlanet', 'CryoSleep', 'Destination', 'VIP', 'Cabin1', 'Cabin3']\nnumerics = ['Age', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']\nidentifiers = ['PassengerID', 'Name', 'Cabin2']\n","metadata":{"execution":{"iopub.status.busy":"2022-06-30T19:06:39.291961Z","iopub.execute_input":"2022-06-30T19:06:39.292346Z","iopub.status.idle":"2022-06-30T19:06:39.318840Z","shell.execute_reply.started":"2022-06-30T19:06:39.292310Z","shell.execute_reply":"2022-06-30T19:06:39.317921Z"},"editable":false,"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"#replace na's with most common for categoricals\nfor i in categoricals:\n    df_train[i].fillna(df_train[i].mode()[0], inplace=True)\n    \n#replace na's with median for numerics\nfor i in numerics:\n    df_train[i].fillna(df_train[i].median(), inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-06-30T19:06:42.522009Z","iopub.execute_input":"2022-06-30T19:06:42.522773Z","iopub.status.idle":"2022-06-30T19:06:42.552591Z","shell.execute_reply.started":"2022-06-30T19:06:42.522736Z","shell.execute_reply":"2022-06-30T19:06:42.551578Z"},"editable":false,"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"#combine costs into 'services\ndf_train['Services'] = df_train['RoomService']+df_train['FoodCourt']+df_train['ShoppingMall']+df_train['Spa']+df_train['VRDeck']","metadata":{"execution":{"iopub.status.busy":"2022-06-30T19:06:44.898034Z","iopub.execute_input":"2022-06-30T19:06:44.898417Z","iopub.status.idle":"2022-06-30T19:06:44.905561Z","shell.execute_reply.started":"2022-06-30T19:06:44.898386Z","shell.execute_reply":"2022-06-30T19:06:44.904450Z"},"editable":false,"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"df_train.loc[df_train['Services'] == 0, 'BoughtServices'] = 0\ndf_train.loc[df_train['Services'] > 0, 'BoughtServices'] = 1","metadata":{"execution":{"iopub.status.busy":"2022-06-30T19:06:46.595314Z","iopub.execute_input":"2022-06-30T19:06:46.596114Z","iopub.status.idle":"2022-06-30T19:06:46.603816Z","shell.execute_reply.started":"2022-06-30T19:06:46.596066Z","shell.execute_reply":"2022-06-30T19:06:46.602561Z"},"editable":false,"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"## Data Munging -  Replacing categoricals with numbers","metadata":{"editable":false}},{"cell_type":"code","source":"#Home Planet\ndf_train['HomePlanet'].replace(['Earth','Europa', 'Mars'],[0,1, 2],inplace=True)\ndf_train['CryoSleep'].replace(['False','True'],[0,1],inplace=True)\ndf_train['Destination'].replace(['TRAPPIST-1e','55 Cancri e','PSO J318.5-22'],[0,1,2],inplace=True)\ndf_train['Cabin1'].replace(['F','G', 'E', 'B', 'C', 'D', 'A', 'T'],[0,1, 2, 3, 4, 5, 6, 7],inplace=True)\ndf_train['Cabin3'].replace(['S','P'],[0,1],inplace=True)\ndf_train['VIP'].replace(['False','True'], [0,1], inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-06-30T19:06:50.666997Z","iopub.execute_input":"2022-06-30T19:06:50.667377Z","iopub.status.idle":"2022-06-30T19:06:50.712940Z","shell.execute_reply.started":"2022-06-30T19:06:50.667347Z","shell.execute_reply":"2022-06-30T19:06:50.711581Z"},"editable":false,"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"df_train.info()","metadata":{"execution":{"iopub.status.busy":"2022-06-30T19:06:55.183716Z","iopub.execute_input":"2022-06-30T19:06:55.184110Z","iopub.status.idle":"2022-06-30T19:06:55.210518Z","shell.execute_reply.started":"2022-06-30T19:06:55.184063Z","shell.execute_reply":"2022-06-30T19:06:55.209341Z"},"editable":false,"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"#drop columns I don't want to use\ndf_train.drop(['PassengerId','Cabin','RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck', 'Name', 'Cabin2' ],axis=1,inplace=True) ","metadata":{"execution":{"iopub.status.busy":"2022-06-30T19:06:58.132894Z","iopub.execute_input":"2022-06-30T19:06:58.133252Z","iopub.status.idle":"2022-06-30T19:06:58.145734Z","shell.execute_reply.started":"2022-06-30T19:06:58.133214Z","shell.execute_reply":"2022-06-30T19:06:58.144877Z"},"editable":false,"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"df_train.info()","metadata":{"execution":{"iopub.status.busy":"2022-06-30T12:56:31.188717Z","iopub.execute_input":"2022-06-30T12:56:31.189353Z","iopub.status.idle":"2022-06-30T12:56:31.199729Z","shell.execute_reply.started":"2022-06-30T12:56:31.18932Z","shell.execute_reply":"2022-06-30T12:56:31.198957Z"},"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Visualization Redux\n\nHow have things changes now that we cleaned up the data?","metadata":{"editable":false}},{"cell_type":"code","source":"df_train.hist(figsize = (20, 10)) ","metadata":{"execution":{"iopub.status.busy":"2022-06-30T12:56:42.198765Z","iopub.execute_input":"2022-06-30T12:56:42.199149Z","iopub.status.idle":"2022-06-30T12:56:43.158896Z","shell.execute_reply.started":"2022-06-30T12:56:42.199117Z","shell.execute_reply":"2022-06-30T12:56:43.157716Z"},"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Munging Conclusion\nI have eliminated all NaN values (in a veryrough way) Split cabin into two categorical variables, created a services column to represent the 5 things people spend money on and dropped several columns. I transformed categoricals into numerical place holders. I have a total of 8 pieces of data aplut our attribute of interest (Transported). ","metadata":{"editable":false}},{"cell_type":"markdown","source":"# Data Transformation\nThe skew on our 'Services' column is terrible, I'll try a logarithmic transform to increase data normality.","metadata":{"editable":false}},{"cell_type":"code","source":"df_train['Services']=np.log(1+df_train['Services'])","metadata":{"execution":{"iopub.status.busy":"2022-06-30T19:07:03.629891Z","iopub.execute_input":"2022-06-30T19:07:03.630731Z","iopub.status.idle":"2022-06-30T19:07:03.638207Z","shell.execute_reply.started":"2022-06-30T19:07:03.630664Z","shell.execute_reply":"2022-06-30T19:07:03.637020Z"},"editable":false,"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"df_train.hist(figsize = (20, 10)) ","metadata":{"execution":{"iopub.status.busy":"2022-06-30T19:07:05.972739Z","iopub.execute_input":"2022-06-30T19:07:05.973253Z","iopub.status.idle":"2022-06-30T19:07:07.102289Z","shell.execute_reply.started":"2022-06-30T19:07:05.973203Z","shell.execute_reply":"2022-06-30T19:07:07.100955Z"},"editable":false,"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{"editable":false}},{"cell_type":"markdown","source":"# Machine Learning\nI will be running the data through several algorithms, using cross validation to score the data, then choosing a couple of the higher performing algorithms to run hyperparameterization on. \n","metadata":{"editable":false}},{"cell_type":"code","source":"X = df_train.drop(columns = 'Transported')\ny = df_train['Transported']","metadata":{"execution":{"iopub.status.busy":"2022-06-30T19:07:50.033826Z","iopub.execute_input":"2022-06-30T19:07:50.034198Z","iopub.status.idle":"2022-06-30T19:07:50.040833Z","shell.execute_reply.started":"2022-06-30T19:07:50.034167Z","shell.execute_reply":"2022-06-30T19:07:50.039956Z"},"editable":false,"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"X.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-30T19:07:53.072710Z","iopub.execute_input":"2022-06-30T19:07:53.073068Z","iopub.status.idle":"2022-06-30T19:07:53.088714Z","shell.execute_reply.started":"2022-06-30T19:07:53.073040Z","shell.execute_reply":"2022-06-30T19:07:53.087562Z"},"editable":false,"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{"editable":false}},{"cell_type":"code","source":"#create lists for relevant model metrics, this will be used to compare different models\nmodel = []\nmodel_mean = []\nmodel_std = []\n\n#k-fold cross validation\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nX = X\ny = y\n\n#folds, adjust based on # of data points\nfolds = 5\nkfold = KFold(n_splits = folds, random_state = 1, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2022-06-30T19:11:06.171657Z","iopub.execute_input":"2022-06-30T19:11:06.172022Z","iopub.status.idle":"2022-06-30T19:11:06.178894Z","shell.execute_reply.started":"2022-06-30T19:11:06.171995Z","shell.execute_reply":"2022-06-30T19:11:06.177764Z"},"editable":false,"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#decision tree classifier\nfrom sklearn.tree import DecisionTreeClassifier\nmethod = DecisionTreeClassifier(random_state = 1) \nresults = cross_val_score(method, X, y, cv=kfold)\nmodel.append(method)\nmodel_mean.append(round(results.mean(), 4))\nmodel_std.append(round(results.std(), 4))\nprint(results.mean())","metadata":{"execution":{"iopub.status.busy":"2022-06-30T19:11:09.330576Z","iopub.execute_input":"2022-06-30T19:11:09.330982Z","iopub.status.idle":"2022-06-30T19:11:09.504376Z","shell.execute_reply.started":"2022-06-30T19:11:09.330951Z","shell.execute_reply":"2022-06-30T19:11:09.503208Z"},"editable":false,"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"#random forest classifier\nfrom sklearn.ensemble import RandomForestClassifier\nmethod = RandomForestClassifier(random_state=1) \nresults = cross_val_score(method, X, y, cv=kfold)\nmodel.append(method)\nmodel_mean.append(round(results.mean(), 4))\nmodel_std.append(round(results.std(), 4))\n","metadata":{"execution":{"iopub.status.busy":"2022-06-30T19:11:11.477851Z","iopub.execute_input":"2022-06-30T19:11:11.478258Z","iopub.status.idle":"2022-06-30T19:11:15.282958Z","shell.execute_reply.started":"2022-06-30T19:11:11.478223Z","shell.execute_reply":"2022-06-30T19:11:15.282023Z"},"editable":false,"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"#gradient boosting classifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nmethod = GradientBoostingClassifier(random_state=1) \nresults = cross_val_score(method, X, y, cv=kfold)\nmodel.append(method)\nmodel_mean.append(round(results.mean(), 4))\nmodel_std.append(round(results.std(), 4))\nprint(results.mean())","metadata":{"execution":{"iopub.status.busy":"2022-06-30T19:11:31.396177Z","iopub.execute_input":"2022-06-30T19:11:31.396601Z","iopub.status.idle":"2022-06-30T19:11:34.187713Z","shell.execute_reply.started":"2022-06-30T19:11:31.396563Z","shell.execute_reply":"2022-06-30T19:11:34.186555Z"},"editable":false,"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"model table\nmodels = pd.DataFrame({\n    'Model' : model,\n    'Score' : model_mean,\n    'std' : model_std\n})\n\n\nmodels.sort_values(by = 'Score', ascending = False)","metadata":{"execution":{"iopub.status.busy":"2022-06-30T19:22:53.480332Z","iopub.execute_input":"2022-06-30T19:22:53.480735Z","iopub.status.idle":"2022-06-30T19:22:53.498084Z","shell.execute_reply.started":"2022-06-30T19:22:53.480694Z","shell.execute_reply":"2022-06-30T19:22:53.496850Z"},"editable":false,"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"markdown","source":"## Machine Learning - Model Choice\nWe tried out three models, the gradient boosting classifier worked the best. Now I want to iterate through important hyper parameters of the model of choice to see if I can improve on it. The parameters to iterate through are particular to each type of model,I chose learning rate, n estimators and max leaf nodes based off of the documentation. These should get me most of the way there.","metadata":{"editable":false}},{"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingClassifier\nbest_score = 0\n\n\nfor learning_rate in [0.1, 1, 10, 100]:\n    for n_estimators in [50, 100, 500, 1000]:\n        for max_leaf_nodes in [2, 4, 5]:\n            gbc = GradientBoostingClassifier(random_state=1, learning_rate=learning_rate, n_estimators=n_estimators, max_leaf_nodes=max_leaf_nodes)\n            scores = cross_val_score(gbc, X, y, cv = 5)\n            score = np.mean(scores)\n            if score > best_score:\n                best_score = score\n                best_std = np.std(scores)\n                best_parameters = {'learning rate':learning_rate, 'n estimators':n_estimators, 'max leaf nodes': max_leaf_nodes}\n                \n                \nprint(best_score)\nprint(best_std)\nprint(best_parameters)","metadata":{"execution":{"iopub.status.busy":"2022-06-30T19:13:28.991715Z","iopub.execute_input":"2022-06-30T19:13:28.992096Z","iopub.status.idle":"2022-06-30T19:20:49.307214Z","shell.execute_reply.started":"2022-06-30T19:13:28.992066Z","shell.execute_reply":"2022-06-30T19:20:49.306129Z"},"editable":false,"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"markdown","source":"## Machine Learning Hyperparameterization\nThat took a long time to iterate through! Out best parameters are leanint rate of 0.1, n estimators of 100, max leaf nodes of 5. But, it only barely improved the predictivity of the model. To be honest, the most bang for my buck would be spending more time data munging, but I want to get an initial model out here first, I can improve upon it later.","metadata":{"editable":false}},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2022-06-30T19:11:58.703731Z","iopub.execute_input":"2022-06-30T19:11:58.704085Z","iopub.status.idle":"2022-06-30T19:11:58.721396Z","shell.execute_reply.started":"2022-06-30T19:11:58.704056Z","shell.execute_reply":"2022-06-30T19:11:58.720371Z"},"editable":false,"trusted":true},"execution_count":33,"outputs":[]}]}